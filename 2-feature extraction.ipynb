{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7F9-XUibZBr"
   },
   "source": [
    "# CNN-LSTM feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10GIIkSiMMqf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import scipy.io\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Directory paths\n",
    "waveform_dir = '/content/final_data_directory'\n",
    "labels_csv_path = \"/content/final_data_directory/class_labels.csv\"\n",
    "\n",
    "\n",
    "def load_data(waveform_dir, labels_csv_path):\n",
    "    labels_df = pd.read_csv(labels_csv_path, header=None, names=['file_name', 'label'])\n",
    "    labels = labels_df['label'].values\n",
    "    file_names = labels_df['file_name'].values\n",
    "\n",
    "    waveforms = []\n",
    "    for file_name in file_names:\n",
    "        mat_path = os.path.join(waveform_dir, file_name + '.mat')\n",
    "        mat_contents = scipy.io.loadmat(mat_path)\n",
    "        waveforms.append(mat_contents['val'][0])  # Assuming 'val' is the correct key\n",
    "\n",
    "    # Determine max length for padding\n",
    "    max_length = max([len(waveform) for waveform in waveforms])\n",
    "\n",
    "    # Pad waveforms\n",
    "    waveforms_padded = pad_sequences(waveforms, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "    # Convert labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "    return np.array(waveforms_padded), labels_categorical, le.classes_\n",
    "\n",
    "\n",
    "waveforms, labels, class_names = load_data(waveform_dir, labels_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Build CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESHum8YkJ3YT",
    "outputId": "8d2087d9-1afe-440b-e232-02f752df43c3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, TimeDistributed, Dropout, Flatten\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'), input_shape=input_shape),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')),\n",
    "        TimeDistributed(MaxPooling1D(pool_size=2, padding='same')),\n",
    "\n",
    "        TimeDistributed(Dropout(0.5)),\n",
    "        TimeDistributed(Flatten()),\n",
    "\n",
    "        LSTM(200, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhxipNJVNQKD",
    "outputId": "28fe9d51-684c-4c88-b05e-68ece90a17cb"
   },
   "outputs": [],
   "source": [
    "# Ensure waveforms have a channel dimension, reshape if necessary\n",
    "if waveforms.ndim == 2:\n",
    "    waveforms = np.expand_dims(waveforms, axis=-1)\n",
    "\n",
    "# Now check the shape\n",
    "print(\"Waveforms shape:\", waveforms.shape)\n",
    "\n",
    "input_shape = waveforms.shape[1:]  # This should now correctly reflect the shape (time steps, 1)\n",
    "model = create_cnn_lstm_model(input_shape, 4)\n",
    "\n",
    "# Assuming you've defined your CNN-LSTM model according to the shapes\n",
    "model.fit(waveforms, labels, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Extract feature from the last layer of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMF-ZJenQWsw",
    "outputId": "e7b35b95-110f-4de8-ec5a-7b713e51966e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# This assumes your model's penultimate layer outputs the 60 features you're interested in\n",
    "feature_layer_name = 'dense'\n",
    "feature_extraction_model = Model(inputs=model.input, outputs=model.get_layer(feature_layer_name).output)\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_and_save(waveform_dir, labels_csv_path, output_csv_path):\n",
    "    labels_df = pd.read_csv(labels_csv_path, header=None, names=['file_name', 'label'])\n",
    "    file_names = labels_df['file_name'].values\n",
    "\n",
    "    features = []  # Placeholder for extracted features\n",
    "\n",
    "    for file_name in file_names:\n",
    "        mat_path = os.path.join(waveform_dir, file_name + '.mat')\n",
    "        mat_contents = scipy.io.loadmat(mat_path)\n",
    "        waveform = mat_contents['val'][0]\n",
    "        # Reshape waveform for the model\n",
    "        waveform = waveform.reshape((1, -1, 1))\n",
    "\n",
    "        # Ensure waveform length matches model's expected input\n",
    "        waveform_padded = pad_sequences(waveform, maxlen=18000, dtype='float32', padding='post').reshape((1, -1, 1))\n",
    "\n",
    "        extracted_features = feature_extraction_model.predict(waveform_padded)\n",
    "        features.append(extracted_features.flatten())  # Flatten the features array\n",
    "\n",
    "    # Prepare headers for the CSV file\n",
    "    headers = ['file_name'] + [f'feature{i+1}' for i in range(60)]\n",
    "\n",
    "    # Convert features to DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "\n",
    "    # Insert the file names at the first column of the DataFrame\n",
    "    features_df.insert(loc=0, column='file_name', value=file_names)\n",
    "\n",
    "    # Name the columns according to headers\n",
    "    features_df.columns = headers\n",
    "\n",
    "    # Save to CSV with headers\n",
    "    features_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Specify your directories and output CSV path\n",
    "waveform_dir = '/content/data/waveforms'\n",
    "labels_csv_path = \"/content/data/labels/labels.csv\"\n",
    "output_csv_path = \"/content/data/features/cnn_lstm_features.csv\"\n",
    "\n",
    "# Extract features and save them\n",
    "extract_features_and_save(waveform_dir, labels_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxlIMnxJbgyq"
   },
   "source": [
    "# 5- Mannual feature extraction of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDWgZULeViTn",
    "outputId": "d0fd8460-3fdc-4f90-c68d-a263c98a07a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biosppy\n",
      "  Downloading biosppy-2.1.2-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/142.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.2/142.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from biosppy) (0.22.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.9.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.11.4)\n",
      "Collecting shortuuid (from biosppy)\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from biosppy) (4.8.0.76)\n",
      "Requirement already satisfied: pywavelets in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (4.48.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->biosppy) (3.2.0)\n",
      "Installing collected packages: shortuuid, biosppy\n",
      "Successfully installed biosppy-2.1.2 shortuuid-1.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install biosppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CYPrw0JDWGa2"
   },
   "outputs": [],
   "source": [
    "# this code is from the paper : https://github.com/Seb-Good/ecg-features\n",
    "\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pywt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from biosppy.signals import ecg\n",
    "from biosppy.signals.tools import filter_signal\n",
    "\n",
    "def hfd(a, k_max):\n",
    "\n",
    "    \"\"\"Compute Higuchi Fractal Dimension of a time series.\"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    L = []\n",
    "    x = []\n",
    "    N = a.size\n",
    "\n",
    "    for k in range(1, k_max):\n",
    "        Lk = 0\n",
    "        for m in range(0, k):\n",
    "            idxs = np.arange(1, int(np.floor((N - m) / k)), dtype=np.int32)\n",
    "            Lmk = np.sum(np.abs(a[m + idxs * k] - a[m + k * (idxs - 1)]))\n",
    "            Lmk = (Lmk * (N - 1) / (((N - m) / k) * k)) / k\n",
    "            Lk += Lmk\n",
    "        L.append(np.log(Lk / (m + 1)))\n",
    "        x.append([np.log(1.0 / k), 1])\n",
    "    (p, r1, r2, s) = np.linalg.lstsq(x, L)\n",
    "\n",
    "    return p[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FullWaveformFeatures:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate a dictionary of full waveform statistics for one ECG signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : numpy array\n",
    "        Full waveform time array.\n",
    "    signal_raw : numpy array\n",
    "        Raw full waveform.\n",
    "    signal_filtered : numpy array\n",
    "        Filtered full waveform.\n",
    "    rpeaks : numpy array\n",
    "        Array indices of R-Peaks\n",
    "    templates_ts : numpy array\n",
    "        Template waveform time array\n",
    "    templates : numpy array\n",
    "        Template waveforms\n",
    "    fs : int, float\n",
    "        Sampling frequency (Hz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_waveform_features : dictionary\n",
    "        Full waveform features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ts, signal_raw, signal_filtered, rpeaks, templates_ts, templates, fs):\n",
    "\n",
    "        # Set parameters\n",
    "        self.ts = ts\n",
    "        self.signal_raw = signal_raw\n",
    "        self.signal_filtered = signal_filtered\n",
    "        self.rpeaks = rpeaks\n",
    "        self.templates_ts = templates_ts\n",
    "        self.templates = templates\n",
    "        self.fs = fs\n",
    "\n",
    "        # Feature dictionary\n",
    "        self.full_waveform_features = dict()\n",
    "\n",
    "    def get_full_waveform_features(self):\n",
    "        return self.full_waveform_features\n",
    "\n",
    "    def extract_full_waveform_features(self):\n",
    "        self.full_waveform_features.update(self.calculate_basic_features())\n",
    "        self.full_waveform_features.update(self.calculate_stationary_wavelet_transform_features())\n",
    "\n",
    "    def calculate_basic_features(self):\n",
    "\n",
    "        # Empty dictionary\n",
    "        basic_features = dict()\n",
    "\n",
    "        # Calculate statistics\n",
    "        basic_features['full_waveform_min'] = np.min(self.signal_filtered)\n",
    "        basic_features['full_waveform_max'] = np.max(self.signal_filtered)\n",
    "        basic_features['full_waveform_mean'] = np.mean(self.signal_filtered)\n",
    "        basic_features['full_waveform_median'] = np.median(self.signal_filtered)\n",
    "        basic_features['full_waveform_std'] = np.std(self.signal_filtered)\n",
    "        basic_features['full_waveform_skew'] = sp.stats.skew(self.signal_filtered)\n",
    "        basic_features['full_waveform_kurtosis'] = sp.stats.kurtosis(self.signal_filtered)\n",
    "        basic_features['full_waveform_duration'] = np.max(self.ts)\n",
    "\n",
    "        return basic_features\n",
    "\n",
    "    def calculate_stationary_wavelet_transform_features(self):\n",
    "\n",
    "        # Empty dictionary\n",
    "        stationary_wavelet_transform_features = dict()\n",
    "\n",
    "        # Decomposition level\n",
    "        decomp_level = 4\n",
    "\n",
    "        # Stationary wavelet transform\n",
    "        swt = self.stationary_wavelet_transform(self.signal_filtered, wavelet='db4', level=decomp_level)\n",
    "\n",
    "        # Set frequency band\n",
    "        freq_band_low = (3, 10)\n",
    "        freq_band_med = (10, 30)\n",
    "        freq_band_high = (30, 45)\n",
    "\n",
    "        \"\"\"Frequency Domain\"\"\"\n",
    "        for level in range(len(swt)):\n",
    "\n",
    "            \"\"\"Detail\"\"\"\n",
    "            # Compute Welch periodogram\n",
    "            fxx, pxx = signal.welch(x=swt[level]['d'], fs=self.fs)\n",
    "\n",
    "            # Get frequency band\n",
    "            freq_band_low_index = np.logical_and(fxx >= freq_band_low[0], fxx < freq_band_low[1])\n",
    "            freq_band_med_index = np.logical_and(fxx >= freq_band_med[0], fxx < freq_band_med[1])\n",
    "            freq_band_high_index = np.logical_and(fxx >= freq_band_high[0], fxx < freq_band_high[1])\n",
    "\n",
    "            # Calculate maximum power\n",
    "            max_power_low = np.max(pxx[freq_band_low_index])\n",
    "            max_power_med = np.max(pxx[freq_band_med_index])\n",
    "            max_power_high = np.max(pxx[freq_band_high_index])\n",
    "\n",
    "            # Calculate average power\n",
    "            mean_power_low = np.trapz(y=pxx[freq_band_low_index], x=fxx[freq_band_low_index])\n",
    "            mean_power_med = np.trapz(y=pxx[freq_band_med_index], x=fxx[freq_band_med_index])\n",
    "            mean_power_high = np.trapz(y=pxx[freq_band_high_index], x=fxx[freq_band_high_index])\n",
    "\n",
    "            # Calculate max/mean power ratio\n",
    "            stationary_wavelet_transform_features['swt_d_' + str(level+1) + '_low_power_ratio'] = \\\n",
    "                max_power_low / mean_power_low\n",
    "            stationary_wavelet_transform_features['swt_d_' + str(level+1) + '_med_power_ratio'] = \\\n",
    "                max_power_med / mean_power_med\n",
    "            stationary_wavelet_transform_features['swt_d_' + str(level+1) + '_high_power_ratio'] = \\\n",
    "                max_power_high / mean_power_high\n",
    "\n",
    "            \"\"\"Approximation\"\"\"\n",
    "            # Compute Welch periodogram\n",
    "            fxx, pxx = signal.welch(x=swt[level]['a'], fs=self.fs)\n",
    "\n",
    "            # Get frequency band\n",
    "            freq_band_low_index = np.logical_and(fxx >= freq_band_low[0], fxx < freq_band_low[1])\n",
    "            freq_band_med_index = np.logical_and(fxx >= freq_band_med[0], fxx < freq_band_med[1])\n",
    "            freq_band_high_index = np.logical_and(fxx >= freq_band_high[0], fxx < freq_band_high[1])\n",
    "\n",
    "            # Calculate maximum power\n",
    "            max_power_low = np.max(pxx[freq_band_low_index])\n",
    "            max_power_med = np.max(pxx[freq_band_med_index])\n",
    "            max_power_high = np.max(pxx[freq_band_high_index])\n",
    "\n",
    "            # Calculate average power\n",
    "            mean_power_low = np.trapz(y=pxx[freq_band_low_index], x=fxx[freq_band_low_index])\n",
    "            mean_power_med = np.trapz(y=pxx[freq_band_med_index], x=fxx[freq_band_med_index])\n",
    "            mean_power_high = np.trapz(y=pxx[freq_band_high_index], x=fxx[freq_band_high_index])\n",
    "\n",
    "            # Calculate max/mean power ratio\n",
    "            stationary_wavelet_transform_features['swt_a_' + str(level+1) + '_low_power_ratio'] = \\\n",
    "                max_power_low / mean_power_low\n",
    "            stationary_wavelet_transform_features['swt_a_' + str(level+1) + '_med_power_ratio'] = \\\n",
    "                max_power_med / mean_power_med\n",
    "            stationary_wavelet_transform_features['swt_a_' + str(level+1) + '_high_power_ratio'] = \\\n",
    "                max_power_high / mean_power_high\n",
    "\n",
    "        \"\"\"Non-Linear\"\"\"\n",
    "        for level in range(len(swt)):\n",
    "\n",
    "            \"\"\"Detail\"\"\"\n",
    "            # Log-energy entropy\n",
    "            stationary_wavelet_transform_features['swt_d_' + str(level+1) + '_energy_entropy'] = \\\n",
    "                np.sum(np.log10(np.power(swt[level]['d'], 2)))\n",
    "\n",
    "            # Higuchi_fractal\n",
    "            stationary_wavelet_transform_features['swt_d_' + str(level+1) + '_higuchi_fractal'] = \\\n",
    "                hfd(swt[level]['d'], k_max=10)\n",
    "\n",
    "            \"\"\"Approximation\"\"\"\n",
    "            # Log-energy entropy\n",
    "            stationary_wavelet_transform_features['swt_a_' + str(level+1) + '_energy_entropy'] = \\\n",
    "                np.sum(np.log10(np.power(swt[level]['a'], 2)))\n",
    "\n",
    "            # Higuchi_fractal\n",
    "            stationary_wavelet_transform_features['swt_a_' + str(level+1) + '_higuchi_fractal'] = \\\n",
    "                hfd(swt[level]['a'], k_max=10)\n",
    "\n",
    "        return stationary_wavelet_transform_features\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_decomposition_level(waveform_length, level):\n",
    "\n",
    "        # Set starting multiplication factor\n",
    "        factor = 0\n",
    "\n",
    "        # Set updated waveform length variable\n",
    "        waveform_length_updated = None\n",
    "\n",
    "        # If waveform is not the correct length for proposed decomposition level\n",
    "        if waveform_length % 2**level != 0:\n",
    "\n",
    "            # Calculate remainder\n",
    "            remainder = waveform_length % 2**level\n",
    "\n",
    "            # Loop through multiplication factors until minimum factor found\n",
    "            while remainder != 0:\n",
    "\n",
    "                # Update multiplication factor\n",
    "                factor += 1\n",
    "\n",
    "                # Update waveform length\n",
    "                waveform_length_updated = factor * waveform_length\n",
    "\n",
    "                # Calculate updated remainder\n",
    "                remainder = waveform_length_updated % 2**level\n",
    "\n",
    "            return waveform_length_updated\n",
    "\n",
    "        # If waveform is the correct length for proposed decomposition level\n",
    "        else:\n",
    "            return waveform_length\n",
    "\n",
    "    @staticmethod\n",
    "    def add_padding(waveform, waveform_length_updated):\n",
    "\n",
    "        # Calculate required padding\n",
    "        pad_count = np.abs(len(waveform) - waveform_length_updated)\n",
    "\n",
    "        # Calculate before waveform padding\n",
    "        pad_before = int(np.floor(pad_count / 2.0))\n",
    "\n",
    "        # Calculate after waveform padding\n",
    "        pad_after = pad_count - pad_before\n",
    "\n",
    "        # Add padding to waveform\n",
    "        waveform_padded = np.append(np.zeros(pad_before), np.append(waveform, np.zeros(pad_after)))\n",
    "\n",
    "        return waveform_padded, pad_before, pad_after\n",
    "\n",
    "    def stationary_wavelet_transform(self, waveform, wavelet, level):\n",
    "\n",
    "        # Calculate waveform length\n",
    "        waveform_length = len(waveform)\n",
    "\n",
    "        # Calculate minimum waveform length for SWT of certain decomposition level\n",
    "        waveform_length_updated = self.calculate_decomposition_level(waveform_length, level)\n",
    "\n",
    "        # Add necessary padding to waveform\n",
    "        waveform_padded, pad_before, pad_after = self.add_padding(waveform, waveform_length_updated)\n",
    "\n",
    "        # Compute stationary wavelet transform\n",
    "        swt = pywt.swtn(waveform_padded, wavelet=wavelet, level=level, start_level=0)\n",
    "\n",
    "        # Loop through decomposition levels and remove padding\n",
    "        for lev in range(len(swt)):\n",
    "\n",
    "            # Approximation\n",
    "            swt[lev]['a'] = swt[lev]['a'][pad_before:len(waveform_padded) - pad_after]\n",
    "\n",
    "            # Detail\n",
    "            swt[lev]['d'] = swt[lev]['d'][pad_before:len(waveform_padded) - pad_after]\n",
    "\n",
    "        return swt\n",
    "\n",
    "\n",
    "\n",
    "class Features:\n",
    "\n",
    "    def __init__(self, file_path, fs, labels=None):\n",
    "\n",
    "        # Set parameters\n",
    "        self.file_path = file_path\n",
    "        self.fs = fs\n",
    "        self.labels = labels\n",
    "\n",
    "        # Set attributes\n",
    "        self.features = None\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "\n",
    "    def extract_features(self, filter_bandwidth, n_signals=None, show=False, labels=None,\n",
    "                         normalize=True, polarity_check=True, template_before=0.2, template_after=0.4):\n",
    "\n",
    "        # Create empty features DataFrame\n",
    "        self.features = pd.DataFrame()\n",
    "\n",
    "        # Get list of .mat files\n",
    "        file_names = self._get_file_names(n_signals=n_signals)\n",
    "\n",
    "        # Loop through .mat files\n",
    "        for file_name in file_names:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Get start time\n",
    "                t_start = time.time()\n",
    "\n",
    "                # Load .mat file\n",
    "                signal_raw = self._load_mat_file(file_name=file_name)\n",
    "\n",
    "                # Preprocess signal\n",
    "                ts, signal_raw, signal_filtered, rpeaks, templates_ts, templates = self._preprocess_signal(\n",
    "                    signal_raw=signal_raw, filter_bandwidth=filter_bandwidth, normalize=normalize,\n",
    "                    polarity_check=polarity_check, template_before=template_before, template_after=template_after\n",
    "                )\n",
    "\n",
    "                # Extract features from waveform\n",
    "                features = self._group_features(file_name=file_name, ts=ts, signal_raw=signal_raw,\n",
    "                                                signal_filtered=signal_filtered, rpeaks=rpeaks,\n",
    "                                                templates_ts=templates_ts, templates=templates,\n",
    "                                                template_before=template_before, template_after=template_after)\n",
    "\n",
    "                # Append feature vector\n",
    "                self.features = self.features.append(features, ignore_index=True)\n",
    "\n",
    "                # Get end time\n",
    "                t_end = time.time()\n",
    "\n",
    "                # Print progress\n",
    "                if show:\n",
    "                    print('Finished extracting features from ' + file_name + '.mat | Extraction time: ' +\n",
    "                          str(np.round((t_end - t_start) / 60, 3)) + ' minutes')\n",
    "\n",
    "            except ValueError:\n",
    "                print('Error loading ' + file_name + '.mat')\n",
    "\n",
    "        # Add labels\n",
    "        self._add_labels(labels=labels)\n",
    "\n",
    "    def _add_labels(self, labels):\n",
    "        \"\"\"Add label to feature DataFrame.\"\"\"\n",
    "        if labels is not None:\n",
    "            self.features = pd.merge(labels, self.features, on='file_name')\n",
    "\n",
    "    def _get_file_names(self, n_signals):\n",
    "        \"\"\"Get list of .mat file names in file path.\"\"\"\n",
    "        file_names = [file.split('.')[0] for file in os.listdir(self.file_path) if file.endswith('.mat')]\n",
    "\n",
    "        return self._get_n_signals(file_names=file_names, n_signals=n_signals)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_n_signals(file_names, n_signals):\n",
    "        \"\"\"Get list of file names equal to n_signals.\"\"\"\n",
    "        if n_signals is not None:\n",
    "            return file_names[0:n_signals]\n",
    "        else:\n",
    "            return file_names\n",
    "\n",
    "    def _load_mat_file(self, file_name):\n",
    "        \"\"\"Loads ECG signal to numpy array from .mat file.\"\"\"\n",
    "        return sio.loadmat(os.path.join(self.file_path, file_name))['val'][0].astype('float')\n",
    "\n",
    "    def _preprocess_signal(self, signal_raw, filter_bandwidth, normalize, polarity_check,\n",
    "                           template_before, template_after):\n",
    "\n",
    "        # Filter signal\n",
    "        signal_filtered = self._apply_filter(signal_raw, filter_bandwidth)\n",
    "\n",
    "        # Get BioSPPy ECG object\n",
    "        ecg_object = ecg.ecg(signal=signal_raw, sampling_rate=self.fs, show=False)\n",
    "\n",
    "        # Get BioSPPy output\n",
    "        ts = ecg_object['ts']          # Signal time array\n",
    "        rpeaks = ecg_object['rpeaks']  # rpeak indices\n",
    "\n",
    "        # Get templates and template time array\n",
    "        templates, rpeaks = self._extract_templates(signal_filtered, rpeaks, template_before, template_after)\n",
    "        templates_ts = np.linspace(-template_before, template_after, templates.shape[1], endpoint=False)\n",
    "\n",
    "        # Polarity check\n",
    "        signal_raw, signal_filtered, templates = self._check_waveform_polarity(polarity_check=polarity_check,\n",
    "                                                                               signal_raw=signal_raw,\n",
    "                                                                               signal_filtered=signal_filtered,\n",
    "                                                                               templates=templates)\n",
    "        # Normalize waveform\n",
    "        signal_raw, signal_filtered, templates = self._normalize_waveform_amplitude(normalize=normalize,\n",
    "                                                                                    signal_raw=signal_raw,\n",
    "                                                                                    signal_filtered=signal_filtered,\n",
    "                                                                                    templates=templates)\n",
    "        return ts, signal_raw, signal_filtered, rpeaks, templates_ts, templates\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_waveform_polarity(polarity_check, signal_raw, signal_filtered, templates):\n",
    "\n",
    "        \"\"\"Invert waveform polarity if necessary.\"\"\"\n",
    "        if polarity_check:\n",
    "\n",
    "            # Get extremes of median templates\n",
    "            templates_min = np.min(np.median(templates, axis=1))\n",
    "            templates_max = np.max(np.median(templates, axis=1))\n",
    "\n",
    "            if np.abs(templates_min) > np.abs(templates_max):\n",
    "                return signal_raw * -1, signal_filtered * -1, templates * -1\n",
    "            else:\n",
    "                return signal_raw, signal_filtered, templates\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_waveform_amplitude(normalize, signal_raw, signal_filtered, templates):\n",
    "        \"\"\"Normalize waveform amplitude by the median R-peak amplitude.\"\"\"\n",
    "        if normalize:\n",
    "\n",
    "            # Get median templates max\n",
    "            templates_max = np.max(np.median(templates, axis=1))\n",
    "\n",
    "            return signal_raw / templates_max, signal_filtered / templates_max, templates / templates_max\n",
    "\n",
    "    def _extract_templates(self, signal_filtered, rpeaks, before, after):\n",
    "\n",
    "        # convert delimiters to samples\n",
    "        before = int(before * self.fs)\n",
    "        after = int(after * self.fs)\n",
    "\n",
    "        # Sort R-Peaks in ascending order\n",
    "        rpeaks = np.sort(rpeaks)\n",
    "\n",
    "        # Get number of sample points in waveform\n",
    "        length = len(signal_filtered)\n",
    "\n",
    "        # Create empty list for templates\n",
    "        templates = []\n",
    "\n",
    "        # Create empty list for new rpeaks that match templates dimension\n",
    "        rpeaks_new = np.empty(0, dtype=int)\n",
    "\n",
    "        # Loop through R-Peaks\n",
    "        for rpeak in rpeaks:\n",
    "\n",
    "            # Before R-Peak\n",
    "            a = rpeak - before\n",
    "            if a < 0:\n",
    "                continue\n",
    "\n",
    "            # After R-Peak\n",
    "            b = rpeak + after\n",
    "            if b > length:\n",
    "                break\n",
    "\n",
    "            # Append template list\n",
    "            templates.append(signal_filtered[a:b])\n",
    "\n",
    "            # Append new rpeaks list\n",
    "            rpeaks_new = np.append(rpeaks_new, rpeak)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        templates = np.array(templates).T\n",
    "\n",
    "        return templates, rpeaks_new\n",
    "\n",
    "    def _apply_filter(self, signal_raw, filter_bandwidth):\n",
    "        \"\"\"Apply FIR bandpass filter to waveform.\"\"\"\n",
    "        signal_filtered, _, _ = filter_signal(signal=signal_raw, ftype='FIR', band='bandpass',\n",
    "                                              order=int(0.3 * self.fs), frequency=filter_bandwidth,\n",
    "                                              sampling_rate=self.fs)\n",
    "        return signal_filtered\n",
    "\n",
    "    def _group_features(self, file_name, ts, signal_raw, signal_filtered, rpeaks,\n",
    "                        templates_ts, templates, template_before, template_after):\n",
    "\n",
    "        \"\"\"Get a dictionary of all ECG features\"\"\"\n",
    "\n",
    "        # Empty features dictionary\n",
    "        features = dict()\n",
    "\n",
    "        # Set ECG file name\n",
    "        features['file_name'] = file_name\n",
    "\n",
    "        # Extract features\n",
    "        full_waveform_features = FullWaveformFeatures(ts=ts, signal_raw=signal_raw,\n",
    "                                                      signal_filtered=signal_filtered, rpeaks=rpeaks,\n",
    "                                                      templates_ts=templates_ts, templates=templates,\n",
    "                                                      fs=self.fs)\n",
    "        full_waveform_features.extract_full_waveform_features()\n",
    "\n",
    "        # Update feature dictionary\n",
    "        features.update(full_waveform_features.get_full_waveform_features())\n",
    "\n",
    "\n",
    "        return pd.Series(data=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k4n01I9oXG-I"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sampling frequency (Hz)\n",
    "fs = 300\n",
    "\n",
    "\n",
    "# Data paths\n",
    "label_path =  \"/content/final_data_directory\"\n",
    "waveform_path = \"/content/final_data_directory\"\n",
    "feature_path = \"/content/final_data_directory/features\"\n",
    "\n",
    "# Read labels CSV\n",
    "labels = pd.read_csv(os.path.join(label_path, 'class_labels.csv'), names=['file_name', 'label'])\n",
    "\n",
    "# Instantiate\n",
    "ecg_features = Features(file_path=waveform_path, fs=fs)\n",
    "\n",
    "# Calculate ECG features\n",
    "ecg_features.extract_features(\n",
    "    filter_bandwidth=[3, 45], n_signals=None, show=True,\n",
    "    labels=labels, normalize=True, polarity_check=True,\n",
    "    template_before=0.25, template_after=0.4\n",
    ")\n",
    "\n",
    "\n",
    "# Get features DataFrame\n",
    "features = ecg_features.get_features()\n",
    "\n",
    "# View DataFrame\n",
    "features.head(10)\n",
    "\n",
    "\n",
    "features.to_csv(os.path.join(feature_path, 'features.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fiMSsgzdLZd"
   },
   "source": [
    "# 6- Merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OipUupnee-DT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "cnn_lstm_features_path = '/content/final_data_directory/features/cnn_lstm_features.csv'\n",
    "features_path = '/content/final_data_directory/features/features.csv'\n",
    "merged_csv_path = '/content/merged_features.csv'\n",
    "\n",
    "# Load the data\n",
    "df_cnn_lstm_features = pd.read_csv(cnn_lstm_features_path)\n",
    "df_features = pd.read_csv(features_path)\n",
    "\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df_features, df_cnn_lstm_features)\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "merged_df.to_csv(merged_csv_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dN5T7y3_lVOA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
