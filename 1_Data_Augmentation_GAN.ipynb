{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pk98BTT_upIV"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/training2017.zip -d ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/REFERENCE.csv'\n",
        "data = pd.read_csv(file_path, header=None, names=['Name', 'Class'])\n",
        "\n",
        "\n",
        "class_distribution = data['Class'].value_counts()\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-LffYPbwndk",
        "outputId": "0c6f51ed-9416-4d9c-d122-65e0276feba6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N    5050\n",
            "O    2456\n",
            "A     738\n",
            "~     284\n",
            "Name: Class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Load Data"
      ],
      "metadata": {
        "id": "hx1vrwrmKeeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "def load_and_crop_ecg_data(class_label, reference_csv_path, data_dir, target_length=9000):\n",
        "    reference_data = pd.read_csv(reference_csv_path, header=None, names=['Name', 'Class'])\n",
        "    class_samples = reference_data[reference_data['Class'] == class_label]\n",
        "\n",
        "    ecg_data_list = []\n",
        "\n",
        "    for _, row in class_samples.iterrows():\n",
        "        file_path = os.path.join(data_dir, f\"{row['Name']}.mat\")\n",
        "        ecg_data = scipy.io.loadmat(file_path)['val'][0]\n",
        "\n",
        "        # Ensure each sequence is exactly target_length long\n",
        "        if len(ecg_data) >= target_length:\n",
        "            ecg_data = ecg_data[:target_length]\n",
        "        else:\n",
        "            # Pad sequences shorter than target_length with zeros\n",
        "            padding = target_length - len(ecg_data)\n",
        "            ecg_data = np.pad(ecg_data, (0, padding), 'constant', constant_values=(0))\n",
        "\n",
        "        ecg_data_list.append(ecg_data)\n",
        "\n",
        "    ecg_data_array = np.array(ecg_data_list)\n",
        "    return ecg_data_array\n",
        "\n"
      ],
      "metadata": {
        "id": "Eh2vtP08wnjU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Build GAN Model"
      ],
      "metadata": {
        "id": "8qaz3qauKdtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "ecg_shape = (9000,)  # Updated to reflect cropped data shape\n",
        "latent_dim = 100  # Example latent dimension for generator input\n",
        "\n",
        "# Generator Model\n",
        "def build_generator(latent_dim):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(256, activation='relu', input_dim=latent_dim),\n",
        "        layers.BatchNormalization(momentum=0.8),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(momentum=0.8),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.BatchNormalization(momentum=0.8),\n",
        "        layers.Dense(np.prod(ecg_shape), activation='tanh'),\n",
        "        layers.Reshape(ecg_shape)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=ecg_shape),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# GAN Model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential([generator, discriminator])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "biEP91qSwno0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Compile the model"
      ],
      "metadata": {
        "id": "eEntLfM1OB4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and compile the discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Initialize the generator\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# The generator is only trained through the GAN model\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5))\n"
      ],
      "metadata": {
        "id": "-wTKR8vBwxfA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Train the model"
      ],
      "metadata": {
        "id": "6cec0e4EOHnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def train_gan(class_data, generator, discriminator, gan, epochs, batch_size, latent_dim):\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        # Randomly select real ECG samples\n",
        "        idx = np.random.randint(0, class_data.shape[0], half_batch)\n",
        "        real_ecgs = class_data[idx]\n",
        "\n",
        "        # Generate fake ECG data\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "        gen_ecgs = generator.predict(noise)\n",
        "\n",
        "        # Labels for real and fake data\n",
        "        real_y = np.ones((half_batch, 1))\n",
        "        fake_y = np.zeros((half_batch, 1))\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_ecgs, real_y)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_ecgs, fake_y)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        valid_y = np.ones((batch_size, 1))  # Labels for generator training\n",
        "\n",
        "        # Train the generator (to have the discriminator label samples as valid)\n",
        "        g_loss = gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n"
      ],
      "metadata": {
        "id": "0-EgyNcWw38D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Generate new data from trained model"
      ],
      "metadata": {
        "id": "oI2Hdd4UORoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_ecg(generator, num_samples, latent_dim):\n",
        "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "    synthetic_ecg = generator.predict(noise)\n",
        "    return synthetic_ecg"
      ],
      "metadata": {
        "id": "1ebiBGJOw3_A"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data of class N"
      ],
      "metadata": {
        "id": "Qf5w7mtkOhKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class_label = 'N'\n",
        "reference_csv_path = '/content/REFERENCE.csv'\n",
        "data_dir = '/content/training2017'\n",
        "\n",
        "first_class_data = load_and_crop_ecg_data(class_label, reference_csv_path, data_dir, target_length=9000)\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"Original data shape of class N:\\n\")\n",
        "print(first_class_data.shape)"
      ],
      "metadata": {
        "id": "Z92HOlr7zOMv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generate new data for class A"
      ],
      "metadata": {
        "id": "EFiCxf3FOY4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class_label = 'A'\n",
        "reference_csv_path = '/content/REFERENCE.csv'\n",
        "data_dir = '/content/training2017'\n",
        "\n",
        "class_data = load_and_crop_ecg_data(class_label, reference_csv_path, data_dir, target_length=9000)\n",
        "\n",
        "epochs = 5  # Number of epochs to train for\n",
        "batch_size = 32  # Size of the batch\n",
        "latent_dim = 100  # Dimensionality of the random noise input to the generator\n",
        "\n",
        "# Assuming `class_data` is your loaded and preprocessed ECG data\n",
        "train_gan(class_data, generator, discriminator, gan, epochs, batch_size, latent_dim)\n",
        "\n",
        "\n",
        "num_samples = 4312\n",
        "latent_dim = 100\n",
        "\n",
        "synthetic_ecg_data = generate_synthetic_ecg(generator, num_samples, latent_dim)\n",
        "\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"synthetic data shape of class A:\\n\")\n",
        "print(synthetic_ecg_data.shape)\n",
        "\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"Original data shape of class A:\\n\")\n",
        "print(class_data.shape)\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"all data shape of class A: \\n\")\n",
        "second_class_data = np.concatenate((synthetic_ecg_data, class_data), axis=0)\n",
        "print(second_class_data.shape)\n"
      ],
      "metadata": {
        "id": "91p0_dbLx0EX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generate new data for class ~\n",
        "\n"
      ],
      "metadata": {
        "id": "nfUnlJpaO1pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class_label = '~'\n",
        "reference_csv_path = '/content/REFERENCE.csv'\n",
        "data_dir = '/content/training2017'\n",
        "\n",
        "class_data = load_and_crop_ecg_data(class_label, reference_csv_path, data_dir, target_length=9000)\n",
        "\n",
        "epochs = 5  # Number of epochs to train for\n",
        "batch_size = 32  # Size of the batch\n",
        "latent_dim = 100  # Dimensionality of the random noise input to the generator\n",
        "\n",
        "# Assuming `class_data` is your loaded and preprocessed ECG data\n",
        "train_gan(class_data, generator, discriminator, gan, epochs, batch_size, latent_dim)\n",
        "\n",
        "\n",
        "num_samples = 4766\n",
        "latent_dim = 100\n",
        "\n",
        "synthetic_ecg_data = generate_synthetic_ecg(generator, num_samples, latent_dim)\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"synthetic data shape of class ~:\\n\")\n",
        "print(synthetic_ecg_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"Original data shape of class ~:\\n\")\n",
        "print(class_data.shape)\n",
        "\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"all data shape of class ~: \\n\")\n",
        "third_class_data = np.concatenate((synthetic_ecg_data, class_data), axis=0)\n",
        "print(third_class_data.shape)"
      ],
      "metadata": {
        "id": "MtpS9rxmyPNv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generate new data for class O"
      ],
      "metadata": {
        "id": "XHHnIZKKO8PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class_label = 'O'\n",
        "reference_csv_path = '/content/REFERENCE.csv'\n",
        "data_dir = '/content/training2017'\n",
        "\n",
        "class_data = load_and_crop_ecg_data(class_label, reference_csv_path, data_dir, target_length=9000)\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "latent_dim = 100\n",
        "\n",
        "train_gan(class_data, generator, discriminator, gan, epochs, batch_size, latent_dim)\n",
        "\n",
        "\n",
        "num_samples = 2594\n",
        "latent_dim = 100\n",
        "\n",
        "synthetic_ecg_data = generate_synthetic_ecg(generator, num_samples, latent_dim)\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"synthetic data shape of class O:\\n\")\n",
        "print(synthetic_ecg_data.shape)\n",
        "\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"Original data shape of class O:\\n\")\n",
        "print(class_data.shape)\n",
        "\n",
        "print(\"\\n\\n ****************************************\")\n",
        "print(\"all data shape of class O: \\n\")\n",
        "fourth_class_data = np.concatenate((synthetic_ecg_data, class_data), axis=0)\n",
        "print(fourth_class_data.shape)"
      ],
      "metadata": {
        "id": "qmc0EJmIzQyR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge all data"
      ],
      "metadata": {
        "id": "R8D0BvQEPAot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  shapes of our class data arrays are as follows\n",
        "# first_class_data.shape -> (5050, 9000) for class 'N'\n",
        "# second_class_data.shape -> (5050, 9000) for class 'A'\n",
        "# third_class_data.shape -> (5050, 9000) for class '~'\n",
        "# fourth_class_data.shape -> (5050, 9000) for class 'O'\n",
        "\n",
        "merge_data = np.concatenate((first_class_data, second_class_data, third_class_data, fourth_class_data), axis=0)\n",
        "print(merge_data.shape)\n",
        "\n",
        "\n",
        "# Create a list of labels corresponding to each class\n",
        "labels = ['N'] * 5050 + ['A'] * 5050 + ['~'] * 5050 + ['O'] * 5050\n",
        "\n",
        "# Create a DataFrame for labels\n",
        "df_labels = pd.DataFrame(labels, columns=['label'])\n",
        "\n",
        "# If you want to include an identifier for each sample, you can do so as follows:\n",
        "df_labels['file_name'] = range(1, len(df_labels) + 1)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_labels.shape)"
      ],
      "metadata": {
        "id": "aPCx4iA20OPk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save all data in new directory"
      ],
      "metadata": {
        "id": "9tj4cZG7PKDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "\n",
        "# Define the new directory path\n",
        "new_dir = '/content/final_data_directory'\n",
        "\n",
        "# Create the new directory if it doesn't already exist\n",
        "if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "\n",
        "# Save each sample in merge_data as a .mat file in the new directory\n",
        "for i, data in enumerate(merge_data):\n",
        "    file_path = os.path.join(new_dir, f'{i+1}.mat')\n",
        "    scipy.io.savemat(file_path, {'data': data.reshape((9000, 1))})  # Reshaping data to match expected input shape for CNN-LSTM\n",
        "\n",
        "# Save the labels DataFrame as a CSV file in the new directory\n",
        "labels_csv_path = os.path.join(new_dir, 'class_labels.csv')\n",
        "df_labels.to_csv(labels_csv_path, index=False)\n",
        "\n",
        "print(f\"All data has been saved to {new_dir}\")\n"
      ],
      "metadata": {
        "id": "7Wfa-6XaBlxa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjvJXapJ4EXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRw0BGVw_2u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-C2UmYRm4Elf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}